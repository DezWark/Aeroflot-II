{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm\n",
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use bonus plates (length of 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "#import qgrid\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#from collections import deque\n",
    "\n",
    "#from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "#from keras.models import Model, load_model, Sequential\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.patches as mpatches\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Definitions\n",
    "#\n",
    "# RED = 0.2\n",
    "# GREEN = 0.4\n",
    "# BLUE = 0.6\n",
    "# PURPLE = 0.8\n",
    "#\n",
    "colors = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "moves = {1: ((0, 0), (1, 0)), 2: ((0, 1), (1, 1)), 3: ((0, 2), (1, 2)), 4: ((0, 3), (1, 3)), 5: ((0, 4), (1, 4)), \n",
    "         6: ((0, 5), (1, 5)), 7: ((1, 0), (2, 0)), 8: ((1, 0), (0, 0)), 9: ((1, 1), (2, 1)), 10: ((1, 1), (0, 1)), \n",
    "         11: ((1, 2), (2, 2)), 12: ((1, 2), (0, 2)), 13: ((1, 3), (2, 3)), 14: ((1, 3), (0, 3)), 15: ((1, 4), (2, 4)), \n",
    "         16: ((1, 4), (0, 4)), 17: ((1, 5), (2, 5)), 18: ((1, 5), (0, 5)), 19: ((2, 0), (3, 0)), 20: ((2, 0), (1, 0)), \n",
    "         21: ((2, 1), (3, 1)), 22: ((2, 1), (1, 1)), 23: ((2, 2), (3, 2)), 24: ((2, 2), (1, 2)), 25: ((2, 3), (3, 3)), \n",
    "         26: ((2, 3), (1, 3)), 27: ((2, 4), (3, 4)), 28: ((2, 4), (1, 4)), 29: ((2, 5), (3, 5)), 30: ((2, 5), (1, 5)), \n",
    "         31: ((3, 0), (4, 0)), 32: ((3, 0), (2, 0)), 33: ((3, 1), (4, 1)), 34: ((3, 1), (2, 1)), 35: ((3, 2), (4, 2)), \n",
    "         36: ((3, 2), (2, 2)), 37: ((3, 3), (4, 3)), 38: ((3, 3), (2, 3)), 39: ((3, 4), (4, 4)), 40: ((3, 4), (2, 4)), \n",
    "         41: ((3, 5), (4, 5)), 42: ((3, 5), (2, 5)), 43: ((4, 0), (5, 0)), 44: ((4, 0), (3, 0)), 45: ((4, 1), (5, 1)), \n",
    "         46: ((4, 1), (3, 1)), 47: ((4, 2), (5, 2)), 48: ((4, 2), (3, 2)), 49: ((4, 3), (5, 3)), 50: ((4, 3), (3, 3)), \n",
    "         51: ((4, 4), (5, 4)), 52: ((4, 4), (3, 4)), 53: ((4, 5), (5, 5)), 54: ((4, 5), (3, 5)), 55: ((5, 0), (6, 0)), \n",
    "         56: ((5, 0), (4, 0)), 57: ((5, 1), (6, 1)), 58: ((5, 1), (4, 1)), 59: ((5, 2), (6, 2)), 60: ((5, 2), (4, 2)), \n",
    "         61: ((5, 3), (6, 3)), 62: ((5, 3), (4, 3)), 63: ((5, 4), (6, 4)), 64: ((5, 4), (4, 4)), 65: ((5, 5), (6, 5)), \n",
    "         66: ((5, 5), (4, 5)), 67: ((6, 0), (5, 0)), 68: ((6, 1), (5, 1)), 69: ((6, 2), (5, 2)), 70: ((6, 3), (5, 3)), \n",
    "         71: ((6, 4), (5, 4)), 72: ((6, 5), (5, 5)), 73: ((0, 0), (0, 1)), 74: ((1, 0), (1, 1)), 75: ((2, 0), (2, 1)), \n",
    "         76: ((3, 0), (3, 1)), 77: ((4, 0), (4, 1)), 78: ((5, 0), (5, 1)), 79: ((6, 0), (6, 1)), 80: ((0, 1), (0, 0)), \n",
    "         81: ((0, 1), (0, 2)), 82: ((1, 1), (1, 0)), 83: ((1, 1), (1, 2)), 84: ((2, 1), (2, 0)), 85: ((2, 1), (2, 2)), \n",
    "         86: ((3, 1), (3, 0)), 87: ((3, 1), (3, 2)), 88: ((4, 1), (4, 0)), 89: ((4, 1), (4, 2)), 90: ((5, 1), (5, 0)), \n",
    "         91: ((5, 1), (5, 2)), 92: ((6, 1), (6, 0)), 93: ((6, 1), (6, 2)), 94: ((0, 2), (0, 1)), 95: ((0, 2), (0, 3)), \n",
    "         96: ((1, 2), (1, 1)), 97: ((1, 2), (1, 3)), 98: ((2, 2), (2, 1)), 99: ((2, 2), (2, 3)), 100: ((3, 2), (3, 1)), \n",
    "         101: ((3, 2), (3, 3)), 102: ((4, 2), (4, 1)), 103: ((4, 2), (4, 3)), 104: ((5, 2), (5, 1)), 105: ((5, 2), (5, 3)), \n",
    "         106: ((6, 2), (6, 1)), 107: ((6, 2), (6, 3)), 108: ((0, 3), (0, 2)), 109: ((0, 3), (0, 4)), 110: ((1, 3), (1, 2)), \n",
    "         111: ((1, 3), (1, 4)), 112: ((2, 3), (2, 2)), 113: ((2, 3), (2, 4)), 114: ((3, 3), (3, 2)), 115: ((3, 3), (3, 4)), \n",
    "         116: ((4, 3), (4, 2)), 117: ((4, 3), (4, 4)), 118: ((5, 3), (5, 2)), 119: ((5, 3), (5, 4)), 120: ((6, 3), (6, 2)), \n",
    "         121: ((6, 3), (6, 4)), 122: ((0, 4), (0, 3)), 123: ((0, 4), (0, 5)), 124: ((1, 4), (1, 3)), 125: ((1, 4), (1, 5)), \n",
    "         126: ((2, 4), (2, 3)), 127: ((2, 4), (2, 5)), 128: ((3, 4), (3, 3)), 129: ((3, 4), (3, 5)), 130: ((4, 4), (4, 3)), \n",
    "         131: ((4, 4), (4, 5)), 132: ((5, 4), (5, 3)), 133: ((5, 4), (5, 5)), 134: ((6, 4), (6, 3)), 135: ((6, 4), (6, 5)), \n",
    "         136: ((0, 5), (0, 4)), 137: ((1, 5), (1, 4)), 138: ((2, 5), (2, 4)), 139: ((3, 5), (3, 4)), 140: ((4, 5), (4, 4)), \n",
    "         141: ((5, 5), (5, 4)), 142: ((6, 5), (6, 4))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "LEARNING_RATE = 0.005\n",
    "UPDATE_TARGET_NET = 1000\n",
    "\n",
    "# Definitions\n",
    "GAMES_TO_PLAY = 120001\n",
    "REPLAY_MEMORY_SIZE = 131072\n",
    "DYNAMIC_LEARNING_EPOCHS = 10\n",
    "MINIBATCH_SIZE = 64\n",
    "NUMBER_OF_MOVES_IN_GAME = 50\n",
    "GAMMA = 0.99\n",
    "NUMBER_OF_STEPS = 2\n",
    "ACTIONS_DIMENSION = 142\n",
    "\n",
    "# Variables\n",
    "MAXIMUM_SCORE = 0\n",
    "TOTAL_SCORE_500 = 0.0\n",
    "AVG_SCORE_HIST = []\n",
    "TOTAL_SUCCESSFUL_MOVES_500 = 0.0\n",
    "AVG_SUCC_MOVES_HIST = []\n",
    "CNN_MOVE_PROB = 0.1\n",
    "CNN_MOVES_COUNT = 0\n",
    "CNN_SUCCESSFUL_PREDICTION = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_fits(field, i, j, new_color):\n",
    "    \"\"\"\n",
    "    Checks if two items to the left or two colors to the top are NOT of the same color as the new item.\n",
    "    Input:\n",
    "    - field: battfield, numpy array\n",
    "    - i, j: position on the new item, int, within field.shape\n",
    "    - new_color: color of the new item, float\n",
    "    Output:\n",
    "    - boolean: True, if the new item is ok\n",
    "    \"\"\"\n",
    "    # Check two colors to the left\n",
    "    if (j > 1):\n",
    "        if (round(field[i, j - 2] % 1.0, 1) == new_color) and (round(field[i, j - 1] % 1.0, 1) == new_color):\n",
    "            return False\n",
    "        \n",
    "    # Check two colors to the right\n",
    "    if (j < 4):\n",
    "        if (round(field[i, j + 2] % 1.0, 1) == new_color) and (round(field[i, j + 1] % 1.0, 1) == new_color):\n",
    "            return False\n",
    "    \n",
    "    # Check two color to the top\n",
    "    if (i < 5):\n",
    "        if (round(field[i + 2, j] % 1.0, 1) == new_color) and (round(field[i + 1, j] % 1.0, 1) == new_color):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_field(field):\n",
    "    \"\"\"\n",
    "    Initialization of the battle field.\n",
    "    Move from bottom left corner and add new elements.\n",
    "    Input: \n",
    "    - field: numpy array of zeros, 7x6\n",
    "    Output:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    \"\"\"\n",
    "    colors = [0.2, 0.4, 0.6, 0.8]\n",
    "    \n",
    "    for i in list(range(field.shape[0]))[::-1]:\n",
    "        for j in range(field.shape[1]):\n",
    "            rd.seed()\n",
    "            new_color = rd.choice(colors)\n",
    "            \n",
    "            while not color_fits(field, i, j, new_color):\n",
    "                rd.seed()\n",
    "                new_color = rd.choice(colors)\n",
    "                \n",
    "            field[i, j] = new_color\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_field(field):\n",
    "    \"\"\"\n",
    "    Visualizes the battle field in colored circles\n",
    "    Handles bonus plates\n",
    "    Input:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    Output:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 7))\n",
    "\n",
    "    ax.set_xlim((0, 10))\n",
    "    ax.set_ylim((0, 13))\n",
    "\n",
    "    circles = []\n",
    "\n",
    "    for ii in range(7):\n",
    "        for jj in range(6):\n",
    "            if (round(field[ii, jj] % 1.0, 1) == 0.2):\n",
    "                clr = \"red\"\n",
    "            elif (round(field[ii, jj] % 1.0, 1) == 0.4):\n",
    "                clr = \"lightgreen\"\n",
    "            elif (round(field[ii, jj] % 1.0, 1) == 0.6):\n",
    "                clr = \"cyan\"\n",
    "            else:\n",
    "                clr = \"purple\"\n",
    "\n",
    "            #if (field[ii, jj] // 1 == 1.0):\n",
    "            #    circles.append( mpatches.RegularPolygon((jj + 1, 7 - ii), numVertices=4, radius=0.4, color=clr) )\n",
    "            #else:\n",
    "            #    circles.append( mpatches.Circle((jj + 1, 7 - ii), radius=0.4, color=clr) )\n",
    "             \n",
    "            #\n",
    "            # DEBUG\n",
    "            #\n",
    "            if (field[ii, jj] == 0.0):\n",
    "                circles.append( mpatches.RegularPolygon((jj + 1, 7 - ii), numVertices=3, radius=0.2, color=\"black\") )\n",
    "            elif (field[ii, jj] // 1 == 1.0):\n",
    "                circles.append( mpatches.RegularPolygon((jj + 1, 7 - ii), numVertices=4, radius=0.4, color=clr) )\n",
    "            else:\n",
    "                circles.append( mpatches.Circle((jj + 1, 7 - ii), radius=0.4, color=clr) )\n",
    "\n",
    "\n",
    "    for circ in circles:\n",
    "        ax.add_artist(circ)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plate_in_set(plate, row, col, length, direction):\n",
    "    \"\"\"\n",
    "    Checks whether plate is with the set given by row, col, length, direction\n",
    "    Input:\n",
    "    - plate: plate location, tuple (row, column)\n",
    "    - row: row where the set starts\n",
    "    - col: column where the set starts\n",
    "    - length: the set's length\n",
    "    - direction: the set's direction\n",
    "    Output:\n",
    "    - True if the plate is within the set, False otherwise\n",
    "    \"\"\"\n",
    "    if (direction == 0):\n",
    "        # Horizontal set\n",
    "        if ((plate[0] != row) or (plate[1] < col) or (plate[1] > (col + length - 1))):\n",
    "            return False\n",
    "    else:\n",
    "        # Vertucal set\n",
    "        if ((plate[1] != col) or (plate[0] < row) or (plate[0] > (row + length - 1))):\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_field(field, colors):\n",
    "    \"\"\"\n",
    "    Наполняет поле после сжигания рядов.\n",
    "    Сдвигает фишки вниз, заполняя верхний ряд каждый раз так, чтобы верхний ряд не создавал халявной тройки\n",
    "    Начинает с левого нижнего угла, чтобы переиспользовать color_fits()\n",
    "    Input:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    - colors: list of 4 floats - color values, see the definition above\n",
    "    Output:\n",
    "    - numpy array of floats, 7x6 - updated field\n",
    "    \"\"\"\n",
    "    for ii in list(range(7))[::-1]:\n",
    "        for jj in range(6):\n",
    "            while (field[ii, jj] == 0.):\n",
    "                # Опускаем на один вниз\n",
    "                # Если мы в самом верхнем ряду, то опускать не нужно\n",
    "                if (ii != 0):\n",
    "                    for iii in list(range(1, ii + 1))[::-1]:\n",
    "                        field[iii, jj] = field[iii - 1, jj]\n",
    "\n",
    "                # Заполняем верх\n",
    "                new_color = rd.choice(colors)\n",
    "\n",
    "                while not color_fits(field, 0, jj, new_color):\n",
    "                    new_color = rd.choice(colors)\n",
    "\n",
    "                field[0, jj] = new_color  \n",
    "                \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move_v2(field, move, moves):\n",
    "    \"\"\"\n",
    "    Physically moves plates according to the move\n",
    "    Input:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    - move: particular move to make, 1<=move<=142\n",
    "    - moves: dictionary of all possible moves defined above\n",
    "    Output:\n",
    "    - new_field: updated field with two swapped plates\n",
    "    - plate_start: coordinates of the plate that started the move, tuple (row, column)\n",
    "    - plate_end: cooredinates of the plate that ended the move, tuple (row, column)\n",
    "    \"\"\"\n",
    "    (start_row, start_col), (end_row, end_col) = moves[move]\n",
    "\n",
    "    # Swap two plates and create new (modified) field\n",
    "    new_field = np.array(field)\n",
    "    temp_color = field[end_row, end_col]\n",
    "    new_field[end_row, end_col] = field[start_row, start_col]\n",
    "    new_field[start_row, start_col] = temp_color\n",
    "        \n",
    "    return new_field, (start_row, start_col), (end_row, end_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(field):\n",
    "    \"\"\"\n",
    "    Finds all sets and all bonus plates included into those sets\n",
    "    Input:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    Output:\n",
    "    - list of sets coordinates: list of tuples (row_start, column_start, set_length, direction, color), counting from top left corner\n",
    "      Direction is either 0 (horizontal) or 1 (vertical)\n",
    "    - list of bonus plates included into sets: list of tuples (row, column, type). \n",
    "      Type is either 4 or 5 (reserved for future)\n",
    "    \"\"\"\n",
    "    perm_bonus_plates = []\n",
    "    perm_sets = []\n",
    "\n",
    "    # Find all 3+ sets in horizontal row\n",
    "    for ii in range(field.shape[0]):\n",
    "        temp_bonus_plates = []\n",
    "        jj = 0\n",
    "        len = 1\n",
    "        while (jj < field.shape[1]):\n",
    "            if (jj > 0):\n",
    "                if (round(field[ii, jj] % 1.0, 1) == round(field[ii, jj - 1] % 1.0, 1)):\n",
    "                    len = len + 1\n",
    "                else:\n",
    "                    if (len >= 3):\n",
    "                        # Add temp list of bonus plates to the permanent list of bonus plates\n",
    "                        perm_bonus_plates = perm_bonus_plates + temp_bonus_plates\n",
    "                        \n",
    "                        # Add to permanent list of sets\n",
    "                        perm_sets.append((ii, jj - len, len, 0, round(field[ii, jj - 1] % 1, 1)))\n",
    "                        \n",
    "                    temp_bonus_plates = []\n",
    "                    len = 1\n",
    "            \n",
    "            if (field[ii, jj] > 1.):\n",
    "                # Add to temp list of bonus plates\n",
    "                temp_bonus_plates.append((ii, jj, 4))\n",
    "            \n",
    "            jj = jj + 1\n",
    "            \n",
    "        if (len >= 3):\n",
    "            # Add temp list of bonus plates to the permanent list of bonus plates\n",
    "            perm_bonus_plates = perm_bonus_plates + temp_bonus_plates\n",
    "\n",
    "            # Add to permanent list of sets\n",
    "            perm_sets.append((ii, jj - len, len, 0, round(field[ii, jj - 1] % 1, 1)))\n",
    "\n",
    "    # Find all 3+ sets in vertical columns\n",
    "    for jj in range(field.shape[1]):\n",
    "        temp_bonus_plates = []\n",
    "        ii = 0\n",
    "        len = 1\n",
    "        while (ii < field.shape[0]):\n",
    "            if (ii > 0):\n",
    "                if (round(field[ii, jj] % 1.0, 1) == round(field[ii - 1, jj] % 1, 1)):\n",
    "                    len = len + 1\n",
    "                else:\n",
    "                    if (len >= 3):\n",
    "                        # Add temp list of bonus plates to the permanent list of bonus plates\n",
    "                        perm_bonus_plates = perm_bonus_plates + temp_bonus_plates\n",
    "                        \n",
    "                        # Add to permanent list of sets\n",
    "                        perm_sets.append((ii - len, jj, len, 1, round(field[ii - 1, jj] % 1, 1)))\n",
    "                        \n",
    "                    temp_bonus_plates = []\n",
    "                    len = 1\n",
    "            \n",
    "            if (field[ii, jj] > 1.):\n",
    "                # Add to temp list of bonus plates\n",
    "                temp_bonus_plates.append((ii, jj, 4))\n",
    "            \n",
    "            ii = ii + 1\n",
    "            \n",
    "        if (len >= 3):\n",
    "            # Add temp list of bonus plates to the permanent list of bonus plates\n",
    "            perm_bonus_plates = perm_bonus_plates + temp_bonus_plates\n",
    "\n",
    "            # Add to permanent list of sets\n",
    "            perm_sets.append((ii - len, jj, len, 1, round(field[ii - 1, jj] % 1, 1)))\n",
    "            \n",
    "    return perm_sets, perm_bonus_plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_v2(field, plate_from, plate_to):\n",
    "    \"\"\"\n",
    "    Calculates the score in the field. \n",
    "    Replaces all sets with zeros.\n",
    "    Handles bonus plates: replaces required rows with zeros (Type 4)\n",
    "    Puts bonus plates, should any set be of the length of 4\n",
    "    Input:\n",
    "    - field: numpy array of floats, 7x6\n",
    "    - plate_from: coordinates of the plate where the move starts, tuple (row, column)\n",
    "    - plate_to: coordinates of the plate where the move ends, tuple (row, column)\n",
    "    Output:\n",
    "    - score: int, 0+\n",
    "    - field: modified field\n",
    "    \"\"\"\n",
    "    # Get all sets with possible bonus plates\n",
    "    sets, bonus_plates = get_sets(field)\n",
    "\n",
    "    # Set all requires plates to zero\n",
    "    #\n",
    "    # First handle sets\n",
    "    for st in sets:\n",
    "        row = st[0]\n",
    "        col = st[1]\n",
    "        lng = st[2]\n",
    "        drc = st[3]\n",
    "\n",
    "        if (drc == 0):\n",
    "            field[row, col:(col + lng)] = 0\n",
    "        else:\n",
    "            field[row:(row + lng), col] = 0\n",
    "    #       \n",
    "    # Then handle bonus plates/rows\n",
    "    for pl in bonus_plates:\n",
    "        row = pl[0]\n",
    "        col = pl[1]\n",
    "        typ = pl[2]\n",
    "\n",
    "        if (typ == 4):\n",
    "            field[row, :] = 0\n",
    "\n",
    "    # Calculate score\n",
    "    score = (field == 0.).sum()\n",
    "\n",
    "    # Put new bonus plates. Specially care for the move coordinates\n",
    "    for st in sets:\n",
    "        row = st[0]\n",
    "        col = st[1]\n",
    "        lng = st[2]\n",
    "        drc = st[3]\n",
    "        clr = st[4]\n",
    "\n",
    "        if (lng >= 4):\n",
    "            if (plate_in_set(plate_from, row, col, lng, drc)):\n",
    "                # Move start plate in set. Put new bonus plate according to the move coordinates\n",
    "                field[plate_from[0], plate_from[1]] = clr + 1.0\n",
    "                \n",
    "                #\n",
    "                # DEBUG\n",
    "                #\n",
    "                #print(\"DEBUG: set of 4+ was made!\")\n",
    "                \n",
    "            elif (plate_in_set(plate_to, row, col, lng, drc)):\n",
    "                # Move end plate in set. Put new bonus plate according to the move coordinates\n",
    "                field[plate_to[0], plate_to[1]] = clr + 1.0\n",
    "                \n",
    "                #\n",
    "                # DEBUG\n",
    "                #\n",
    "                #print(\"DEBUG: set of 4+ was made!\")\n",
    "                \n",
    "            else:\n",
    "                # Just put the new bonus plate at the very right/bottom of the set\n",
    "                # This CANNOT happen during the manual move!\n",
    "                # It CAN ONLY HAPPEN when the field is randomly filled with new plates\n",
    "                if (drc == 0):\n",
    "                    field[row, col + lng - 1] = clr + 1.0\n",
    "                else:\n",
    "                    field[row + lng - 1, col] = clr + 1.0\n",
    "\n",
    "    return score, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_max_score(field, aero_cnn, number_of_moves, moves, debug_flag = False):\n",
    "    \"\"\"\n",
    "    TODO \n",
    "    \"\"\"\n",
    "    X_data = ()\n",
    "           \n",
    "    for move in range(1, number_of_moves + 1):\n",
    "        swapped, _, _ = make_move_v2(np.array(field), move, moves)\n",
    "        X_data = X_data + ( swapped, )\n",
    "    \n",
    "    # Теперь выбираем наиболее успешый ход из всех успешных\n",
    "    X_data = np.expand_dims(np.stack(X_data, axis=0), axis=3)\n",
    "    \n",
    "    prediction = aero_cnn.predict(X_data)\n",
    "    \n",
    "    if (debug_flag):\n",
    "        print(\"DEBUG\", prediction)\n",
    "    \n",
    "    best_move_score = prediction.max()\n",
    "    best_move_number = prediction.argmax() + 1\n",
    "    \n",
    "    return best_move_score, best_move_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_entropy(logits):\n",
    "    '''\n",
    "    Softmax Entropy\n",
    "    '''\n",
    "    return tf.reduce_sum(tf.nn.softmax(logits, axis=-1) * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rews, gamma):\n",
    "    '''\n",
    "    Discounted reward to go \n",
    "    Parameters:\n",
    "    ----------\n",
    "    rews: list of rewards\n",
    "    gamma: discount value \n",
    "    '''\n",
    "    rtg = np.zeros_like(rews, dtype=np.float32)\n",
    "    rtg[-1] = rews[-1]\n",
    "    for i in reversed(range(len(rews)-1)):\n",
    "        rtg[i] = rews[i] + gamma*rtg[i+1]\n",
    "    return rtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    '''\n",
    "    Buffer class to store the experience from a unique policy\n",
    "    '''\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.obs = []\n",
    "        self.act = []\n",
    "        self.ret = []\n",
    "\n",
    "        \n",
    "    def store(self, temp_traj):\n",
    "        '''\n",
    "        Add temp_traj values to the buffers and compute the advantage and reward to go\n",
    "        Parameters:\n",
    "        -----------\n",
    "        temp_traj: list where each element is a list that contains: observation, reward, action, state-value\n",
    "        '''\n",
    "        # store only if the temp_traj list is not empty\n",
    "        if len(temp_traj) > 0:\n",
    "            self.obs.extend(temp_traj[:,0])\n",
    "            rtg = discounted_rewards(temp_traj[:,1], self.gamma)\n",
    "            self.ret.extend(rtg)\n",
    "            self.act.extend(temp_traj[:,2])\n",
    "\n",
    "            \n",
    "    def get_batch(self):\n",
    "        b_ret = self.ret\n",
    "        return self.obs, self.act, b_ret\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert(len(self.obs) == len(self.act) == len(self.ret))\n",
    "        return len(self.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Algorithm on TensorFlow starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REINFORCE Algorithm\n",
    "Parameters:\n",
    "-----------\n",
    "env_name: Name of the environment\n",
    "hidden_size: list of the number of hidden units for each layer\n",
    "lr: policy learning rate\n",
    "gamma: discount factor\n",
    "steps_per_epoch: number of steps per epoch\n",
    "num_epochs: number train epochs (Note: they aren't properly epochs)\n",
    "'''\n",
    "\n",
    "#\n",
    "# Convolutional Neural Network\n",
    "#\n",
    "Aero_CNN = tf.keras.models.Sequential()\n",
    "\n",
    "Aero_CNN.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides = (1, 1), \n",
    "                                    padding='same', \n",
    "                                    activation=tf.keras.activations.relu, \n",
    "                                    data_format = 'channels_last',\n",
    "                                    input_shape=(7, 6, 1)))\n",
    "Aero_CNN.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides = (1, 1), \n",
    "                                    padding='same', \n",
    "                                    activation=tf.keras.activations.relu))\n",
    "Aero_CNN.add(tf.keras.layers.Flatten())\n",
    "Aero_CNN.add(tf.keras.layers.Dense(units=ACTIONS_DIMENSION, activation=tf.keras.activations.tanh))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New maximum: 71, after 0 games.\n",
      "New maximum: 75, after 2 games.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         if all(\n\u001b[0m\u001b[1;32m    853\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             for tensor in nest.flatten(node.input_tensors)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# main cycle\n",
    "for game in range(GAMES_TO_PLAY):\n",
    "    # Start one game\n",
    "    game_score = 0\n",
    "    successful_moves = 0\n",
    "\n",
    "    # initialize environment for the new epochs\n",
    "    field = np.zeros((7, 6))\n",
    "    field = initialize_field(field)\n",
    "\n",
    "    # intiaizlie buffer and other variables for the new epochs\n",
    "    buffer = Buffer(GAMMA)\n",
    "    env_buf = []\n",
    "\n",
    "    for m in range(NUMBER_OF_MOVES_IN_GAME):\n",
    "        # Total score of one move\n",
    "        rew = 0            \n",
    "\n",
    "        # run the policy: predict next move\n",
    "        X_data = np.expand_dims(np.expand_dims(field, axis=0), axis=3)\n",
    "        p_logits_one_move = Aero_CNN(X_data)\n",
    "        act = tf.squeeze(tf.random.categorical(p_logits_one_move, 1))\n",
    "\n",
    "        # take a step in the environment            \n",
    "        new_field, plate_a, plate_b = make_move_v2(np.copy(field), act.numpy() + 1, moves)\n",
    "\n",
    "        # Calculate the score and update the field\n",
    "        score, new_field = calculate_score_v2(new_field, plate_a, plate_b)\n",
    "\n",
    "        # If the move is successful, update the field and check if there are new color sets\n",
    "        # While there are new color sets, process them and move plates\n",
    "        successful_move_flag = False\n",
    "\n",
    "        while (score > 0.):\n",
    "            if (not successful_move_flag):\n",
    "                successful_moves = successful_moves + 1\n",
    "                successful_move_flag = True\n",
    "\n",
    "            # Add new points to total move score\n",
    "            rew = rew + score\n",
    "\n",
    "            # Move plates downward, filling the upper row so, that it doesn't have \"easy\" sets of three\n",
    "            # Start from the left lower corner (to reuse color_fits())\n",
    "            new_field = fill_field(new_field, colors)\n",
    "\n",
    "            # Get score and process new possible color sets\n",
    "            score, new_field = calculate_score_v2(new_field, (-1, -1), (-1, -1))\n",
    "\n",
    "        # Increase total game score\n",
    "        game_score = game_score + rew\n",
    "\n",
    "        # add the new transition\n",
    "        env_buf.append([np.copy(field), rew, act])\n",
    "        \n",
    "        # If move is successful, update the play field\n",
    "        if (successful_move_flag):\n",
    "            field = np.copy(new_field)\n",
    "\n",
    "    # store the trajectory just completed\n",
    "    buffer.store(np.array(env_buf))\n",
    "    \n",
    "    # collect the episodes' information\n",
    "    obs_batch, act_batch, ret_batch = buffer.get_batch()\n",
    "\n",
    "    for _ in range(DYNAMIC_LEARNING_EPOCHS):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ##################################################\n",
    "            ########### COMPUTE THE LOSS FUNCTIONS ###########\n",
    "            ##################################################\n",
    "            \n",
    "            # Prepare model input to be in correct shape\n",
    "            X_data = np.expand_dims(np.stack(obs_batch, axis=0), axis=3)\n",
    "\n",
    "            # policy: output layer of CNN, size 142 (number of possible actions)\n",
    "            p_logits = Aero_CNN(X_data)\n",
    "\n",
    "            act_multn = tf.squeeze(tf.random.categorical(p_logits, 1))\n",
    "            actions_mask = tf.one_hot(act_batch, depth=ACTIONS_DIMENSION)\n",
    "\n",
    "            p_log = tf.reduce_sum(actions_mask * tf.nn.log_softmax(p_logits), axis=1)\n",
    "\n",
    "            loss = -tf.reduce_mean(p_log*ret_batch)\n",
    "    \n",
    "        gradients = tape.gradient(loss, Aero_CNN.trainable_variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, Aero_CNN.trainable_variables))\n",
    "\n",
    "    #\n",
    "    # Calculate and display overall stats\n",
    "    #\n",
    "    # Check where we have new record\n",
    "    if (game_score > MAXIMUM_SCORE):\n",
    "        print(f\"New maximum: {game_score}, after {game} games.\")\n",
    "        MAXIMUM_SCORE = game_score\n",
    "\n",
    "    # After each five hundred games output average game score, average number of successful moves per game\n",
    "    TOTAL_SCORE_500 = TOTAL_SCORE_500 + game_score\n",
    "    TOTAL_SUCCESSFUL_MOVES_500 = TOTAL_SUCCESSFUL_MOVES_500 + successful_moves\n",
    "\n",
    "    if ((game % 500 == 0) and (game > 0)):\n",
    "        avg_score = TOTAL_SCORE_500 / 500\n",
    "        TOTAL_SCORE_500 = 0.0\n",
    "\n",
    "        avg_succ_moves = TOTAL_SUCCESSFUL_MOVES_500 / 500\n",
    "        TOTAL_SUCCESSFUL_MOVES_500 = 0.0\n",
    "\n",
    "        print(f\"Games: {game}, last 500 games avg score: {avg_score}, avg of successful moves: {avg_succ_moves}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New maximum: 101, after 0 games.\n",
      "New maximum: 114, after 1 games.\n",
      "New maximum: 125, after 9 games.\n",
      "New maximum: 134, after 15 games.\n",
      "New maximum: 143, after 123 games.\n",
      "New maximum: 164, after 230 games.\n",
      "New maximum: 190, after 479 games.\n",
      "Games: 500, last 500 games avg score: 62.532, avg of successful moves: 9.428, loss 313510336.0\n",
      "CNN made 2587 moves. Successful were 333\n",
      "New maximum: 218, after 595 games.\n",
      "Games: 1000, last 500 games avg score: 58.364, avg of successful moves: 8.708, loss 19710588485632.0\n",
      "CNN made 4988 moves. Successful were 475\n",
      "Games: 1500, last 500 games avg score: 57.842, avg of successful moves: 8.938, loss 2873663296634880.0\n",
      "CNN made 5072 moves. Successful were 548\n",
      "Games: 2000, last 500 games avg score: 58.35, avg of successful moves: 8.83, loss 2.661239905006387e+16\n",
      "CNN made 5004 moves. Successful were 448\n",
      "Games: 2500, last 500 games avg score: 60.308, avg of successful moves: 8.886, loss 1.0147208988144435e+17\n",
      "CNN made 4818 moves. Successful were 465\n",
      "Games: 3000, last 500 games avg score: 56.246, avg of successful moves: 8.52, loss 2.3491306445602816e+17\n",
      "CNN made 5063 moves. Successful were 469\n",
      "Games: 3500, last 500 games avg score: 57.954, avg of successful moves: 8.912, loss 4.47921486464811e+17\n",
      "CNN made 4973 moves. Successful were 475\n",
      "Games: 4000, last 500 games avg score: 59.708, avg of successful moves: 8.932, loss 4.717059501981696e+17\n",
      "CNN made 4942 moves. Successful were 502\n",
      "Games: 4500, last 500 games avg score: 56.386, avg of successful moves: 8.68, loss 6.297127117767311e+17\n",
      "CNN made 5023 moves. Successful were 474\n",
      "New maximum: 229, after 4633 games.\n",
      "Games: 5000, last 500 games avg score: 60.212, avg of successful moves: 8.966, loss 8.33613844612907e+17\n",
      "CNN made 4991 moves. Successful were 505\n",
      "Games: 5500, last 500 games avg score: 58.554, avg of successful moves: 8.764, loss 9.569191945851372e+17\n",
      "CNN made 5108 moves. Successful were 461\n",
      "Games: 6000, last 500 games avg score: 58.404, avg of successful moves: 8.842, loss 8.155146462691328e+17\n",
      "CNN made 5097 moves. Successful were 512\n",
      "Games: 6500, last 500 games avg score: 57.742, avg of successful moves: 8.564, loss 8.907118957601751e+17\n",
      "CNN made 4900 moves. Successful were 468\n",
      "Games: 7000, last 500 games avg score: 57.968, avg of successful moves: 8.692, loss 9.734359895381115e+17\n",
      "CNN made 5001 moves. Successful were 492\n",
      "Games: 7500, last 500 games avg score: 60.75, avg of successful moves: 9.166, loss 8.918592361437594e+17\n",
      "CNN made 4952 moves. Successful were 538\n",
      "Games: 8000, last 500 games avg score: 56.89, avg of successful moves: 8.566, loss 1.1886446373932892e+18\n",
      "CNN made 4980 moves. Successful were 476\n",
      "Games: 8500, last 500 games avg score: 57.182, avg of successful moves: 8.808, loss 1.1891993410095022e+18\n",
      "CNN made 5027 moves. Successful were 470\n",
      "Games: 9000, last 500 games avg score: 58.844, avg of successful moves: 8.846, loss 1.2410253713405379e+18\n",
      "CNN made 4931 moves. Successful were 481\n",
      "Games: 9500, last 500 games avg score: 57.242, avg of successful moves: 8.784, loss 9.018044562559468e+17\n",
      "CNN made 5074 moves. Successful were 466\n",
      "Games: 10000, last 500 games avg score: 59.68, avg of successful moves: 8.878, loss 1.0441895627854971e+18\n",
      "CNN made 4984 moves. Successful were 498\n",
      "Games: 10500, last 500 games avg score: 57.562, avg of successful moves: 8.854, loss 1.1104730715101594e+18\n",
      "CNN made 4981 moves. Successful were 493\n",
      "Games: 11000, last 500 games avg score: 58.292, avg of successful moves: 8.78, loss 1.0084558471996375e+18\n",
      "CNN made 4904 moves. Successful were 488\n",
      "Games: 11500, last 500 games avg score: 58.88, avg of successful moves: 8.85, loss 8.89158698146988e+17\n",
      "CNN made 5020 moves. Successful were 505\n",
      "Games: 12000, last 500 games avg score: 58.742, avg of successful moves: 8.886, loss 9.886419604723466e+17\n",
      "CNN made 4955 moves. Successful were 454\n",
      "Games: 12500, last 500 games avg score: 57.25, avg of successful moves: 8.578, loss 7.594025821740728e+17\n",
      "CNN made 4863 moves. Successful were 422\n",
      "Games: 13000, last 500 games avg score: 58.688, avg of successful moves: 8.728, loss 1.040928548736467e+18\n",
      "CNN made 5117 moves. Successful were 449\n",
      "Games: 13500, last 500 games avg score: 58.706, avg of successful moves: 8.722, loss 6.46593513797976e+17\n",
      "CNN made 5112 moves. Successful were 480\n",
      "Games: 14000, last 500 games avg score: 58.242, avg of successful moves: 8.678, loss 6.314204594930975e+17\n",
      "CNN made 4961 moves. Successful were 435\n",
      "Games: 14500, last 500 games avg score: 58.68, avg of successful moves: 8.686, loss 7.0367754617199e+17\n",
      "CNN made 5053 moves. Successful were 431\n",
      "Games: 15000, last 500 games avg score: 56.636, avg of successful moves: 8.708, loss 5.868622010216612e+17\n",
      "CNN made 4941 moves. Successful were 453\n",
      "Games: 15500, last 500 games avg score: 55.728, avg of successful moves: 8.494, loss 7.658633124988846e+17\n",
      "CNN made 5059 moves. Successful were 489\n",
      "Games: 16000, last 500 games avg score: 58.096, avg of successful moves: 8.868, loss 5.241740953198264e+17\n",
      "CNN made 5089 moves. Successful were 458\n",
      "Games: 16500, last 500 games avg score: 55.164, avg of successful moves: 8.42, loss 4.736154582982328e+17\n",
      "CNN made 5078 moves. Successful were 427\n",
      "Games: 17000, last 500 games avg score: 57.8, avg of successful moves: 8.702, loss 5.31110708021035e+17\n",
      "CNN made 4978 moves. Successful were 437\n",
      "Games: 17500, last 500 games avg score: 58.438, avg of successful moves: 8.704, loss 6.042111888379085e+17\n",
      "CNN made 5095 moves. Successful were 426\n",
      "Games: 18000, last 500 games avg score: 58.564, avg of successful moves: 8.89, loss 5.037156199007519e+17\n",
      "CNN made 5061 moves. Successful were 435\n",
      "Games: 18500, last 500 games avg score: 58.378, avg of successful moves: 8.8, loss 6.812536937182659e+17\n",
      "CNN made 5014 moves. Successful were 440\n",
      "Games: 19000, last 500 games avg score: 58.434, avg of successful moves: 8.606, loss 5.911941393961452e+17\n",
      "CNN made 4986 moves. Successful were 394\n",
      "Games: 19500, last 500 games avg score: 56.644, avg of successful moves: 8.532, loss 3.9170761446496666e+17\n",
      "CNN made 5058 moves. Successful were 427\n",
      "Games: 20000, last 500 games avg score: 58.43, avg of successful moves: 8.786, loss 4.8207908649199206e+17\n",
      "CNN made 5049 moves. Successful were 464\n",
      "Games: 20500, last 500 games avg score: 56.122, avg of successful moves: 8.368, loss 4.657474218093445e+17\n",
      "CNN made 5023 moves. Successful were 457\n",
      "Games: 21000, last 500 games avg score: 59.762, avg of successful moves: 8.734, loss 4.0843661516210176e+17\n",
      "CNN made 4875 moves. Successful were 423\n",
      "Games: 21500, last 500 games avg score: 59.954, avg of successful moves: 9.11, loss 3.7431739496267776e+17\n",
      "CNN made 4961 moves. Successful were 458\n",
      "Games: 22000, last 500 games avg score: 58.152, avg of successful moves: 8.602, loss 3.9792019875928474e+17\n",
      "CNN made 4991 moves. Successful were 467\n",
      "Games: 22500, last 500 games avg score: 58.878, avg of successful moves: 9.034, loss 2.8195604740754637e+17\n",
      "CNN made 5049 moves. Successful were 523\n",
      "Games: 23000, last 500 games avg score: 57.174, avg of successful moves: 8.644, loss 3.4079627472902554e+17\n",
      "CNN made 5092 moves. Successful were 456\n",
      "Games: 23500, last 500 games avg score: 57.128, avg of successful moves: 8.696, loss 3.6993611600389734e+17\n",
      "CNN made 5058 moves. Successful were 460\n",
      "Games: 24000, last 500 games avg score: 58.968, avg of successful moves: 8.83, loss 3.321865489277256e+17\n",
      "CNN made 5034 moves. Successful were 479\n",
      "Games: 24500, last 500 games avg score: 58.628, avg of successful moves: 8.874, loss 3.5395292778777805e+17\n",
      "CNN made 4944 moves. Successful were 508\n",
      "Games: 25000, last 500 games avg score: 57.12, avg of successful moves: 8.536, loss 2.645901717798912e+17\n",
      "CNN made 5061 moves. Successful were 465\n",
      "Games: 25500, last 500 games avg score: 57.172, avg of successful moves: 8.672, loss 2.89270342352896e+17\n",
      "CNN made 5042 moves. Successful were 476\n",
      "Games: 26000, last 500 games avg score: 57.274, avg of successful moves: 8.74, loss 3.193278978798387e+17\n",
      "CNN made 5032 moves. Successful were 484\n",
      "Games: 26500, last 500 games avg score: 59.498, avg of successful moves: 9.188, loss 3.633506941885481e+17\n",
      "CNN made 5007 moves. Successful were 553\n",
      "Games: 27000, last 500 games avg score: 57.506, avg of successful moves: 8.824, loss 3.6071341247011226e+17\n",
      "CNN made 5099 moves. Successful were 497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games: 27500, last 500 games avg score: 56.55, avg of successful moves: 8.604, loss 3.047152852674806e+17\n",
      "CNN made 5024 moves. Successful were 485\n",
      "Games: 28000, last 500 games avg score: 60.15, avg of successful moves: 9.062, loss 2.7861847986143232e+17\n",
      "CNN made 5063 moves. Successful were 482\n",
      "Games: 28500, last 500 games avg score: 58.298, avg of successful moves: 8.76, loss 2.906416051514245e+17\n",
      "CNN made 5002 moves. Successful were 467\n",
      "Games: 29000, last 500 games avg score: 56.662, avg of successful moves: 8.686, loss 3.080669746660639e+17\n",
      "CNN made 4995 moves. Successful were 469\n",
      "Games: 29500, last 500 games avg score: 59.038, avg of successful moves: 8.96, loss 3.632824213884109e+17\n",
      "CNN made 5029 moves. Successful were 454\n",
      "Games: 30000, last 500 games avg score: 57.19, avg of successful moves: 8.702, loss 2.6198523975512883e+17\n",
      "CNN made 5019 moves. Successful were 472\n",
      "Games: 30500, last 500 games avg score: 59.88, avg of successful moves: 9.01, loss 3.0928279400821555e+17\n",
      "CNN made 4862 moves. Successful were 485\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Number of moves made to follow the target CNN update strategy\n",
    "total_moves = 1\n",
    "                                                      \n",
    "for game in range(GAMES_TO_PLAY):\n",
    "    # Start one game\n",
    "    game_score = 0\n",
    "    successful_moves = 0\n",
    "\n",
    "    # Initialize the game field\n",
    "    field = np.zeros((7, 6))\n",
    "    field = initialize_field(field)\n",
    "\n",
    "    for m in range(NUMBER_OF_MOVES_IN_GAME):\n",
    "        # Total score of one move\n",
    "        reward = 0\n",
    "\n",
    "        # Whether CNN made the move\n",
    "        cnn_made_move_flag = False\n",
    "        \n",
    "        # If replay_memory has less than MINIBATCH_SIZE + NUMBER_OF_STEPS moves, then make a random move\n",
    "        if ((len(replay_memory) < MINIBATCH_SIZE + NUMBER_OF_STEPS) or (rd.random() > CNN_MOVE_PROB)):\n",
    "            move = rd.randint(1, 142)\n",
    "        else:\n",
    "            # CNN selects a move\n",
    "            cnn_made_move_flag = True\n",
    "            X_data = np.expand_dims(np.expand_dims(field, axis=0), axis=3)\n",
    "            move = cnn_target.predict(X_data).argmax() + 1\n",
    "            \n",
    "        # Make the move\n",
    "        s_before = field\n",
    "        new_field, plate_a, plate_b = make_move_v2(field, move, moves)\n",
    "\n",
    "        # Calculate the score and update the field\n",
    "        score, new_field = calculate_score_v2(new_field, plate_a, plate_b)\n",
    "        \n",
    "        # Если ход результативный, то обновляем поле и проверяем, получились ли новые цветовые ряды\n",
    "        # Пока есть новые цветные ряды, обрабатываем их, обсчитываем и сдвигаем блюда\n",
    "        successful_move_flag = False\n",
    "\n",
    "        while (score > 0.):\n",
    "            if (not successful_move_flag):\n",
    "                successful_moves = successful_moves + 1\n",
    "                successful_move_flag = True\n",
    "\n",
    "            # Суммируем набранные очки\n",
    "            reward = reward + score\n",
    "\n",
    "            # Сдвигаем фишки вниз, заполняя верхний ряд каждый раз так, чтобы верхний ряд не создавал халявной тройки\n",
    "            # Начинаем с левого нижнего угла (чтобы переиспользовать color_fits())\n",
    "            new_field = fill_field(new_field, colors)\n",
    "\n",
    "            # Считаем очки и обрабатываем новые возможные цветные ряды\n",
    "            score, new_field = calculate_score_v2(new_field, (-1, -1), (-1, -1))\n",
    "\n",
    "        # Увеличиваем счет игры\n",
    "        game_score = game_score + reward\n",
    "        \n",
    "        # Update CNN move statistics\n",
    "        if (cnn_made_move_flag):\n",
    "            if (successful_move_flag):\n",
    "                CNN_SUCCESSFUL_PREDICTION = CNN_SUCCESSFUL_PREDICTION + 1\n",
    "                \n",
    "            CNN_MOVES_COUNT = CNN_MOVES_COUNT + 1\n",
    "            \n",
    "        # Check whether it's the last move of the current game\n",
    "        last_move = m == NUMBER_OF_MOVES_IN_GAME - 1\n",
    "            \n",
    "        #\n",
    "        # Train CNN based on the score\n",
    "        #\n",
    "        if (len(replay_memory) >= MINIBATCH_SIZE + NUMBER_OF_STEPS):\n",
    "            # Select random MINIBATCH_SIZE moves from replay memory buffer\n",
    "            samples = replay_memory.sample_minibatch(MINIBATCH_SIZE)\n",
    "\n",
    "            # Prepare some things for training\n",
    "            S_before = np.expand_dims(samples[0], axis=3)\n",
    "            S_after = np.expand_dims(samples[3], axis=3)            \n",
    "\n",
    "            # Update online CNN weights: training step\n",
    "            loss = cnn_online.train(S_before, samples[1], samples[2], S_after, samples[4], cnn_target)\n",
    "        \n",
    "        # Add new move to the replay memory\n",
    "        replay_memory.add(field, move, reward, new_field, last_move)\n",
    "        \n",
    "        # If move is successful, update the play field\n",
    "        if (successful_move_flag):\n",
    "            field = new_field\n",
    "            \n",
    "        # After each 1000 moves update target CNN\n",
    "        if (total_moves % UPDATE_TARGET_NET == 0):\n",
    "            cnn_target.set_weights(cnn_online.get_weights())\n",
    "            \n",
    "        total_moves = total_moves + 1\n",
    "\n",
    "    #\n",
    "    # Calculate and display overall stats\n",
    "    #\n",
    "    # Проверяем, не обновили ли максимум\n",
    "    if (game_score > MAXIMUM_SCORE):\n",
    "        print(f\"New maximum: {game_score}, after {game} games.\")\n",
    "        MAXIMUM_SCORE = game_score\n",
    "        \n",
    "    # After each hundred of games output average game score, average number of successful moves per game\n",
    "    TOTAL_SCORE_100 = TOTAL_SCORE_100 + game_score\n",
    "    TOTAL_SUCCESSFUL_MOVES_100 = TOTAL_SUCCESSFUL_MOVES_100 + successful_moves\n",
    "    \n",
    "    if ((game % 500 == 0) and (game > 0)):\n",
    "        avg_score = TOTAL_SCORE_100 / 500\n",
    "        #AVG_SCORE_HIST.append(avg_score)\n",
    "        TOTAL_SCORE_100 = 0.0\n",
    "        \n",
    "        avg_succ_moves = TOTAL_SUCCESSFUL_MOVES_100 / 500\n",
    "        #AVG_SUCC_MOVES_HIST.append(avg_succ_moves)\n",
    "        TOTAL_SUCCESSFUL_MOVES_100 = 0.0\n",
    "\n",
    "        print(f\"Games: {game}, last 500 games avg score: {avg_score}, avg of successful moves: {avg_succ_moves}, loss {loss}\")        \n",
    "\n",
    "        print(f\"CNN made {CNN_MOVES_COUNT} moves. Successful were {CNN_SUCCESSFUL_PREDICTION}\")\n",
    "        \n",
    "        if (CNN_SUCCESSFUL_PREDICTION / CNN_MOVES_COUNT >= CNN_MOVE_PROB):\n",
    "            CNN_MOVE_PROB = CNN_MOVE_PROB + 0.1\n",
    "            \n",
    "        CNN_MOVES_COUNT = 0\n",
    "        CNN_SUCCESSFUL_PREDICTION = 0\n",
    "\n",
    "    # After each 500 games check the CNN move stats and increase the prob if appropriate\n",
    "    #if ((game % 100 == 0) and (game > 0)):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "print(MAXIMUM_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
